{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "63a168da-bd09-4fda-959c-021ec54ed921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import re\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#pip install distance\n",
    "import distance\n",
    "# This package is used for finding longest common subsequence between two strings\n",
    "# you can write your own dp code for this\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "# pip install fuzzywuzzy\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.manifold import TSNE\n",
    "# Import the Required lib packages for WORD-Cloud generation\n",
    "# https://stackoverflow.com/questions/45625434/how-to-install-wordcloud-in-python3-6\n",
    "#pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "\n",
    "## Read data\n",
    "# df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "## Basic Stats and Data Visualization\n",
    "\n",
    "## Filling the null values with ' '\n",
    "# df = df.fillna('')\n",
    "\n",
    "## Basic Feature Extraction\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    df = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count') \n",
    "    df['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count')\n",
    "    df['q1len'] = df['question1'].str.len() \n",
    "    df['q2len'] = df['question2'].str.len()\n",
    "    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "\n",
    "    def normalized_word_Common(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)\n",
    "    df['word_Common'] = df.apply(normalized_word_Common, axis=1)\n",
    "\n",
    "    def normalized_word_Total(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * (len(w1) + len(w2))\n",
    "    df['word_Total'] = df.apply(normalized_word_Total, axis=1)\n",
    "\n",
    "    def normalized_word_share(row):\n",
    "        w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "        w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n",
    "        return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "    df['word_share'] = df.apply(normalized_word_share, axis=1)\n",
    "\n",
    "    df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2']\n",
    "    df['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2'])\n",
    "\n",
    "    df.to_csv(\"df_fe_without_preprocessing_train.csv\", index=False)\n",
    "\n",
    "## Analyse Basic Features\n",
    "\n",
    "## Preprocessing of text\n",
    "\n",
    "SAFE_DIV = 0.0001 \n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "def preprocess(x):\n",
    "    x = str(x).lower()\n",
    "    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n",
    "                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
    "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
    "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
    "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
    "                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n",
    "                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\")\n",
    "    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n",
    "    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    pattern = re.compile('\\W')\n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = re.sub(pattern, ' ', x)\n",
    "    \n",
    "    if type(x) == type(''):\n",
    "        x = porter.stem(x)\n",
    "        example1 = BeautifulSoup(x)\n",
    "        x = example1.get_text()\n",
    "\n",
    "    return x\n",
    "\n",
    "    \n",
    "## Advanced Feature Extraction (NLP & Fuzzy Features)\n",
    "\n",
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0]*10\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    #Average Token Length of both Questions\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    return token_features\n",
    "\n",
    "# get the Longest Common sub string\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "\n",
    "def extract_features(df):\n",
    "    # preprocessing each question\n",
    "    df[\"question1\"] = df[\"question1\"].fillna(\"\").apply(preprocess)\n",
    "    df[\"question2\"] = df[\"question2\"].fillna(\"\").apply(preprocess)\n",
    "\n",
    "    print(\"token features...\")\n",
    "    \n",
    "    # Merging Features with dataset\n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    \n",
    "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "   \n",
    "    #Computing Fuzzy Features and Merging with Dataset\n",
    "    \n",
    "    # do read this blog: http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\n",
    "    # https://stackoverflow.com/questions/31806695/when-to-use-which-fuzz-function-to-compare-2-strings\n",
    "    # https://github.com/seatgeek/fuzzywuzzy\n",
    "    print(\"fuzzy features..\")\n",
    "\n",
    "    df[\"token_set_ratio\"]       = df.apply(lambda x: fuzz.token_set_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n",
    "    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n",
    "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
    "    return df\n",
    "    \n",
    "if os.path.isfile('nlp_features_train.csv'):\n",
    "    df = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
    "    df.fillna('')\n",
    "    # df.fillnan('')\n",
    "else:\n",
    "    print(\"Extracting features for train:\")\n",
    "    df = pd.read_csv(\"train.csv\")\n",
    "    df = extract_features(df)    #486sec\n",
    "    df.to_csv(\"nlp_features_train.csv\", index=False)\n",
    "\n",
    "## Analyse Advanced Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9068899a-7806-442c-8716-797754040b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 8182416 stored elements and shape (808580, 86198)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q_mean W2V\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# df.head()\n",
    "# merge texts\n",
    "questions = list(df['question1'].fillna('')) + list(df['question2'].fillna(''))\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, )\n",
    "# questions\n",
    "tfidf.fit_transform(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a520e80-65a7-4d2d-87ec-c8d975419e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3d8cc402-8a47-445d-aa9a-5b29f7fd8b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy\n",
    "import spacy\n",
    "# After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.\n",
    "# here we use a pre-trained GLOVE model which comes free with \"Spacy\". https://spacy.io/usage/vectors-similarity\n",
    "# It is trained on Wikipedia and therefore, it is stronger in terms of word semantics.\n",
    "# en_vectors_web_lg, which includes over 1 million unique vectors.\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "580dec8a-64b2-4932-b678-12055ac80c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                                        | 3320/404290 [00:35<1:05:20, 102.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                      | 13032/404290 [02:19<1:06:23, 98.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|███▊                                                                     | 20807/404290 [03:50<1:10:05, 91.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|█████████████████▋                                                        | 96738/404290 [18:24<49:49, 102.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|███████████████████                                                       | 104115/404290 [19:44<50:43, 98.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|████████████████████████▌                                                 | 134416/404290 [25:03<45:00, 99.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████████████▊                                      | 189667/404290 [47:18<1:23:44, 42.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|█████████████████████████████████▉                                      | 190574/404290 [47:40<1:20:55, 44.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|████████████████████████████████████                                  | 208489/404290 [1:16:04<1:19:11, 41.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|████████████████████████████████████▉                                 | 213229/404290 [1:17:57<1:15:09, 42.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|█████████████████████████████████████████████▎                          | 254168/404290 [1:37:31<56:32, 44.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|████████████████████████████████████████████████▋                       | 273070/404290 [1:41:00<27:31, 79.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████▋                  | 301585/404290 [1:46:44<17:41, 96.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████████████████████████████████▋       | 363378/404290 [1:59:20<07:10, 95.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████▋| 402437/404290 [2:06:47<00:18, 101.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 404290/404290 [2:07:06<00:00, 53.01it/s]\n"
     ]
    }
   ],
   "source": [
    "vecs1 = []         #2HRS_7MINS\n",
    "from tqdm import tqdm\n",
    "# https://github.com/noamraph/tqdm\n",
    "# tqdm is used to print the progress bar\n",
    "for qu1 in tqdm(list(df['question1'])):\n",
    "    # This should be handled during data cleaning\n",
    "    if type(qu1) is float:\n",
    "        # print(qu1)\n",
    "        qu1=''\n",
    "    doc1 = nlp(qu1) \n",
    "    \n",
    "    # 384 is the number of dimensions of vectors \n",
    "    # Changed to 96. Think, change in dimension of vector coz of changed version of spacy\n",
    "    mean_vec1 = np.zeros([len(doc1), 96])\n",
    "    for word1 in doc1:\n",
    "        # word2vec\n",
    "        vec1 = word1.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word1)]\n",
    "        except:\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec1 += vec1 * idf\n",
    "    mean_vec1 = mean_vec1.mean(axis=0)\n",
    "    vecs1.append(mean_vec1)\n",
    "df['q1_feats_m'] = list(vecs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9ffe6c2a-9f01-4bb4-9edf-dc1fe20d39f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 404290/404290 [3:14:26<00:00, 34.65it/s]\n"
     ]
    }
   ],
   "source": [
    "vecs2 = []    #3HRS 14MINS\n",
    "for qu2 in tqdm(list(df['question2'])):\n",
    "    if type(qu2) is float:\n",
    "        qu2=''\n",
    "    doc2 = nlp(qu2) \n",
    "    mean_vec2 = np.zeros([len(doc2), 96])\n",
    "    for word2 in doc2:\n",
    "        # word2vec\n",
    "        vec2 = word2.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word2)]\n",
    "        except:\n",
    "            #print word\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec2 += vec2 * idf\n",
    "    mean_vec2 = mean_vec2.mean(axis=0)\n",
    "    vecs2.append(mean_vec2)\n",
    "df['q2_feats_m'] = list(vecs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a236d57-d0f3-49f6-9719-4e0782460766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepro_features_train.csv (Simple Preprocessing Feartures)\n",
    "#nlp_features_train.csv (NLP Features)\n",
    "if os.path.isfile('nlp_features_train.csv'):\n",
    "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n",
    "\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8165c13e-a8a1-4c83-82f7-8b7e67b972be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
    "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\n",
    "df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aeeb9526-07ab-40d1-ab56-c618eb0b6c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in nlp dataframe : 17\n",
      "Number of features in preprocessed dataframe : 12\n",
      "Number of features in question1 w2v  dataframe : 96\n",
      "Number of features in question2 w2v  dataframe : 96\n",
      "Number of features in final dataframe  : 221\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features in nlp dataframe :\", df1.shape[1])\n",
    "print(\"Number of features in preprocessed dataframe :\", df2.shape[1])\n",
    "print(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\n",
    "print(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\n",
    "print(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5bc62f6-8f5f-47b2-99ab-b5d123113d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the final features to csv file\n",
    "if not os.path.isfile('final_features.csv'):\n",
    "    df3_q1['id']=df1['id']\n",
    "    df3_q2['id']=df1['id']\n",
    "    df1  = df1.merge(df2, on='id',how='left')\n",
    "    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n",
    "    result  = df1.merge(df2, on='id',how='left')\n",
    "    result.to_csv('final_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac7483-8f61-4930-bca0-fefa3cf0c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ML Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
